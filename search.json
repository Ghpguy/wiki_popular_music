[
  {
    "objectID": "quarto.html",
    "href": "quarto.html",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "",
    "text": "This report analyzes the Billboard Top Ten singles dataset, exploring trends in song longevity, peak positions, artist performance, and collaboration networks over time. The analysis leverages Python, pandas, and Plotly for data visualization and lifelines for survival analysis.\n\n\nThe data was scraped and pre-processed from Billboard’s published Top Ten singles charts.\nThe data was collected using a custom Python scraper, available here: get_data.py on GitHub.\nPreprocessing steps included: - Parsing chart dates and normalizing artist names - Handling collaborations and featured artists - Calculating song entry, peak, and longevity metrics - Removing duplicates and correcting known data errors The resulting CSV contains columns for song name, artist(s), entry/peak dates, peak position, weeks in top ten, and more.\nView the raw CSV on GitHub\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom lifelines import KaplanMeierFitter\nfrom sklearn.cluster import KMeans\nimport networkx as nx\nimport calendar\nfrom pathlib import Path\nimport re\n\n# Load data\nrepo_root = Path().resolve()\ncsv_path = repo_root / \"data\" / \"billboard_data_2025_09.csv\"\ndf = pd.read_csv(csv_path)\ndf['Top Ten Entry Date'] = pd.to_datetime(df['Top Ten Entry Date'], errors='coerce')\ndf['Peak Date'] = pd.to_datetime(df['Peak Date'], errors='coerce')\ndf['Decade'] = (df['Top Ten Entry Date'].dt.year // 10) * 10\ndf.head()\n\n\n\n\n\n\n\n\n\nTop Ten Entry Date\nSingle Name\nArtist(s)\nPeak\nPeak Date\nWeeks in Top Ten\nRef\nYear\nDecade\n\n\n\n\n0\n1960-12-12\nWonderland by Night\nBert Kaempfert\n1\n1961-01-09\n10\nNaN\n1960\n1960\n\n\n1\n1960-12-12\nExodus\nFerrante & Teicher\n2\n1961-01-23\n11\nNaN\n1960\n1960\n\n\n2\n1960-12-26\nCorrina, Corinna\nRay Peterson\n9\n1961-01-09\n5\nNaN\n1960\n1960\n\n\n3\n1960-12-31\nAngel Baby\nRosie and the Originals\n5\n1961-01-23\n7\nNaN\n1960\n1960\n\n\n4\n1961-01-09\nWill You Love Me Tomorrow\nThe Shirelles\n1\n1961-01-30\n7\nNaN\n1961\n1960"
  },
  {
    "objectID": "quarto.html#data-loading-and-preprocessing",
    "href": "quarto.html#data-loading-and-preprocessing",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "",
    "text": "The data was scraped and pre-processed from Billboard’s published Top Ten singles charts.\nThe data was collected using a custom Python scraper, available here: get_data.py on GitHub.\nPreprocessing steps included: - Parsing chart dates and normalizing artist names - Handling collaborations and featured artists - Calculating song entry, peak, and longevity metrics - Removing duplicates and correcting known data errors The resulting CSV contains columns for song name, artist(s), entry/peak dates, peak position, weeks in top ten, and more.\nView the raw CSV on GitHub\n\n\nCode\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nfrom lifelines import KaplanMeierFitter\nfrom sklearn.cluster import KMeans\nimport networkx as nx\nimport calendar\nfrom pathlib import Path\nimport re\n\n# Load data\nrepo_root = Path().resolve()\ncsv_path = repo_root / \"data\" / \"billboard_data_2025_09.csv\"\ndf = pd.read_csv(csv_path)\ndf['Top Ten Entry Date'] = pd.to_datetime(df['Top Ten Entry Date'], errors='coerce')\ndf['Peak Date'] = pd.to_datetime(df['Peak Date'], errors='coerce')\ndf['Decade'] = (df['Top Ten Entry Date'].dt.year // 10) * 10\ndf.head()\n\n\n\n\n\n\n\n\n\nTop Ten Entry Date\nSingle Name\nArtist(s)\nPeak\nPeak Date\nWeeks in Top Ten\nRef\nYear\nDecade\n\n\n\n\n0\n1960-12-12\nWonderland by Night\nBert Kaempfert\n1\n1961-01-09\n10\nNaN\n1960\n1960\n\n\n1\n1960-12-12\nExodus\nFerrante & Teicher\n2\n1961-01-23\n11\nNaN\n1960\n1960\n\n\n2\n1960-12-26\nCorrina, Corinna\nRay Peterson\n9\n1961-01-09\n5\nNaN\n1960\n1960\n\n\n3\n1960-12-31\nAngel Baby\nRosie and the Originals\n5\n1961-01-23\n7\nNaN\n1960\n1960\n\n\n4\n1961-01-09\nWill You Love Me Tomorrow\nThe Shirelles\n1\n1961-01-30\n7\nNaN\n1961\n1960"
  },
  {
    "objectID": "quarto.html#overall-distribution",
    "href": "quarto.html#overall-distribution",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Overall Distribution",
    "text": "Overall Distribution\n\n\nCode\nprint(f\"The overall average number of weeks a song spends in the top 10 is {df['Weeks in Top Ten'].mean():.2f} weeks\")\nfig = px.bar(\n  df['Weeks in Top Ten'].value_counts().sort_index().reset_index(),\n  x='Weeks in Top Ten',\n  y='count',\n  text_auto=True,\n  title=\"Overall Distribution of Weeks in Top Ten (Exact Weeks)\"\n)\nfig.show()\n\n\nThe overall average number of weeks a song spends in the top 10 is 6.64 weeks"
  },
  {
    "objectID": "quarto.html#by-decade",
    "href": "quarto.html#by-decade",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "By Decade",
    "text": "By Decade\nIt seems that by decade the general trend is that songs are lasting longer on the top ten charts. Popular sentiment finds that songs have been lasting longer on the charts, making it harder for newer songs / artists to compete in the space.\n\n\nCode\ndecade_stats = df.groupby('Decade', as_index=False)['Weeks in Top Ten'].mean()\nfig = px.bar(\n  decade_stats,\n  x='Decade',\n  y='Weeks in Top Ten',\n  text_auto='.2f',\n  title=\"Average Weeks in Top Ten by Decade\"\n)\nfig.show()"
  },
  {
    "objectID": "quarto.html#by-year",
    "href": "quarto.html#by-year",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "By Year",
    "text": "By Year\n\n\nCode\ntrend = df.groupby(\"Year\")[\"Weeks in Top Ten\"].mean().reset_index()\nfig = px.line(\n  trend,\n  x=\"Year\",\n  y=\"Weeks in Top Ten\",\n  title=\"Average Weeks in Top Ten Over Time Per Year\",\n  labels={\"Weeks in Top Ten\": \"Avg Weeks\", \"Year\": \"Year\"}\n)\nfig.show()"
  },
  {
    "objectID": "quarto.html#survival-curve-all-songs",
    "href": "quarto.html#survival-curve-all-songs",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Survival Curve (All Songs)",
    "text": "Survival Curve (All Songs)\n\n\nCode\nkmf = KaplanMeierFitter()\nkmf.fit(df[\"Weeks in Top Ten\"], event_observed=[1]*len(df))\nsurvival_df = kmf.survival_function_.reset_index()\nsurvival_df.columns = [\"Weeks in Top Ten\", \"Probability of Staying\"]\nfig = px.line(\n  survival_df,\n  x=\"Weeks in Top Ten\",\n  y=\"Probability of Staying\",\n  title=\"Survival Curve of Billboard Top Ten Singles\",\n  labels={\"Weeks in Top Ten\": \"Weeks in Top Ten\", \"Probability of Staying\": \"Probability of Staying in Top Ten\"}\n)\nfig.show()"
  },
  {
    "objectID": "quarto.html#survival-curves-by-decade",
    "href": "quarto.html#survival-curves-by-decade",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Survival Curves by Decade",
    "text": "Survival Curves by Decade\n\n\nCode\nfig = go.Figure()\nkmf = KaplanMeierFitter()\n\n# Only unique decades\ndecades = sorted(df['Decade'].dropna().unique())\n\nfor i, decade in enumerate(decades):\n    group = df[df['Decade'] == decade]\n    if len(group) == 0:\n        continue\n\n    # Fit Kaplan-Meier curve\n    kmf.fit(group[\"Weeks in Top Ten\"], event_observed=[1]*len(group))\n    surv = kmf.survival_function_.reset_index()\n    surv.columns = [\"Weeks in Top Ten\", \"Probability of Staying\"]\n\n    # Add survival curve\n    fig.add_trace(\n        go.Scatter(\n            x=surv[\"Weeks in Top Ten\"],\n            y=surv[\"Probability of Staying\"],\n            mode=\"lines\",\n            name=str(decade),\n            line=dict(width=3)\n        )\n    )\n\n    # Automatically pick evenly spaced points along the middle 80% of the curve\n    n_labels = 1\n    start_idx = int(len(surv) * 0.3)   # skip first 10%\n    end_idx = int(len(surv) * 0.95)     # skip last 10%\n    indices = np.linspace(start_idx, end_idx, n_labels, dtype=int)\n\n    for j, idx in enumerate(indices):\n        # stagger labels vertically to reduce overlap\n        y_offset = 15 * (i - len(decades)/2) + 10 * (j - (n_labels-1)/2)\n\n        # Add annotation once per point\n        fig.add_annotation(\n            x=surv[\"Weeks in Top Ten\"].iloc[idx],\n            y=surv[\"Probability of Staying\"].iloc[idx],\n            text=f\"{decade}\",\n            showarrow=True,\n            arrowhead=2,\n            font=dict(size=12),\n            ax=0,\n            ay=-y_offset\n        )\n\n# Layout\nfig.update_layout(\n    title=\"Survival Curves of Billboard Top Ten Singles by Decade\",\n    xaxis_title=\"Weeks in Top Ten\",\n    yaxis_title=\"Probability of Staying in Top Ten\",\n    yaxis=dict(range=[0, 1]),\n    showlegend=True\n)\n\nfig.show()"
  },
  {
    "objectID": "quarto.html#distribution-of-peak-positions",
    "href": "quarto.html#distribution-of-peak-positions",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Distribution of Peak Positions",
    "text": "Distribution of Peak Positions\n\n\nCode\nfig = px.bar(\n  df['Peak'].value_counts().sort_index().reset_index(),\n  x='Peak',\n  y='count',\n  text_auto=True,\n  title=\"Overall Distribution of Peak Positions\"\n)\nfig.show()"
  },
  {
    "objectID": "quarto.html#average-peak-by-decade",
    "href": "quarto.html#average-peak-by-decade",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Average Peak by Decade",
    "text": "Average Peak by Decade\n\n\nCode\ndecade_stats = df.groupby('Decade', as_index=False)['Peak'].mean()\nfig = px.bar(\n  decade_stats,\n  x='Decade',\n  y='Peak',\n  text_auto='.2f',\n  title=\"Average Peak Position in Top Ten by Decade\"\n)\nfig.show()"
  },
  {
    "objectID": "quarto.html#average-peak-by-year",
    "href": "quarto.html#average-peak-by-year",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Average Peak by Year",
    "text": "Average Peak by Year\n\n\nCode\ntrend = df.groupby(\"Year\")[\"Peak\"].mean().reset_index()\nfig = px.line(\n  trend,\n  x=\"Year\",\n  y=\"Peak\",\n  title=\"Average Peak Position in Top Ten Over Time Per Year\",\n  labels={\"Peak\": \"Avg Peak Position\", \"Year\": \"Year\"}\n)\nfig.show()"
  },
  {
    "objectID": "quarto.html#top-10-artists-by-top-ten-entries",
    "href": "quarto.html#top-10-artists-by-top-ten-entries",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Top 10 Artists by Top Ten Entries",
    "text": "Top 10 Artists by Top Ten Entries\n\n\nCode\ntop_artists = df[\"Artist(s)\"].value_counts().reset_index()\ntop_artists.columns = [\"Artist\", \"Entries\"]\ntop_artists = top_artists.sort_values(\"Entries\", ascending=False).head(10)\nfig = px.bar(\n  top_artists,\n  x=\"Entries\",\n  y=\"Artist\",\n  orientation=\"h\",\n  text=\"Entries\",\n  title=\"Top Artists by Top Ten Entries\"\n)\nfig.show()"
  },
  {
    "objectID": "quarto.html#top-10-artists-by-top-ten-real-estate",
    "href": "quarto.html#top-10-artists-by-top-ten-real-estate",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Top 10 Artists by Top Ten Real Estate",
    "text": "Top 10 Artists by Top Ten Real Estate\nThis chart shows the top artists by the total amount of weekly space they have occupied on the top ten charts\n\n\nCode\n# Calculate total weeks in Top Ten for each artist\nartist_real_estate = df.groupby(\"Artist(s)\")[\"Weeks in Top Ten\"].sum().reset_index()\nartist_real_estate = artist_real_estate.sort_values(\"Weeks in Top Ten\", ascending=False).head(10)\n\n# Plot horizontal bar chart\nfig = px.bar(\n    artist_real_estate,\n    x=\"Weeks in Top Ten\",\n    y=\"Artist(s)\",\n    orientation=\"h\",\n    text=\"Weeks in Top Ten\",\n    title=\"Top 10 Artists by Total Weeks in Top Ten (Chart Real Estate)\"\n)\n\nfig.update_traces(textposition=\"outside\")\nfig.update_layout(\n    xaxis_title=\"Total Weeks in Top Ten\",\n    yaxis_title=\"Artist\",\n    yaxis=dict(autorange=\"reversed\")\n)\n\nfig.show()"
  },
  {
    "objectID": "quarto.html#one-hit-wonders",
    "href": "quarto.html#one-hit-wonders",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "One-Hit Wonders",
    "text": "One-Hit Wonders\nWhich artists make one entry on the top ten charts but never return?\n\n\nCode\nartist_song_counts = df.groupby(\"Artist(s)\")[\"Single Name\"].nunique()\none_hit_pct = (artist_song_counts == 1).mean() * 100\nprint(f\"One-hit wonders (Artists that reach the top 10 only once): {one_hit_pct:.1f}% of artists\")\n\n\nOne-hit wonders (Artists that reach the top 10 only once): 70.2% of artists"
  },
  {
    "objectID": "quarto.html#top-10-songs-with-greatest-lag-to-peak",
    "href": "quarto.html#top-10-songs-with-greatest-lag-to-peak",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Top 10 Songs with Greatest Lag to Peak",
    "text": "Top 10 Songs with Greatest Lag to Peak\n\n\nCode\ntop_lag = df.nlargest(10, \"Lag to Peak\")[[\"Single Name\", \"Artist(s)\", \"Lag to Peak\", \"Top Ten Entry Date\", \"Peak Date\"]]\ntop_lag['Label'] = top_lag[\"Single Name\"] + \" – \" + top_lag[\"Artist(s)\"]\nfig_top_lag = px.bar(\n  top_lag,\n  x=\"Lag to Peak\",\n  y=\"Label\",\n  orientation='h',\n  text=\"Lag to Peak\",\n  hover_data={\n    \"Lag to Peak\": True,\n    \"Top Ten Entry Date\": True,\n    \"Peak Date\": True,\n    \"Label\": False\n  },\n  title=\"Top 10 Songs with Greatest Lag to Peak\"\n)\nfig_top_lag.show()"
  },
  {
    "objectID": "quarto.html#sleeper-hits-cluster-2",
    "href": "quarto.html#sleeper-hits-cluster-2",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "Sleeper Hits (Cluster 2)",
    "text": "Sleeper Hits (Cluster 2)\n\n\nCode\ncluster_2_songs = df[df[\"Cluster\"] == \"2\"][[\"Single Name\", \"Artist(s)\", \"Weeks in Top Ten\", \"Lag to Peak\", \"Peak\"]]\ncluster_2_songs_sorted = cluster_2_songs.sort_values(by=[\"Peak\", \"Weeks in Top Ten\"], ascending=[True, False])\ncluster_2_songs_sorted['Label'] = cluster_2_songs_sorted[\"Single Name\"] + \" – \" + cluster_2_songs_sorted[\"Artist(s)\"]\nfig_cluster2 = px.bar(\n  cluster_2_songs_sorted,\n  x=\"Weeks in Top Ten\",\n  y=\"Label\",\n  orientation='h',\n  text=\"Weeks in Top Ten\",\n  hover_data=[\"Peak\", \"Lag to Peak\"],\n  title=\"Sleeper Hits (Songs that Peaked Late but Stayed Long)\",\n  height=800\n)\nfig_cluster2.show()"
  },
  {
    "objectID": "quarto.html#jupyter-python3",
    "href": "quarto.html#jupyter-python3",
    "title": "Billboard Top Ten Analysis (1958 - 2025)",
    "section": "jupyter: python3",
    "text": "jupyter: python3"
  },
  {
    "objectID": "notebooks/scraper.html",
    "href": "notebooks/scraper.html",
    "title": "Billboard Top Ten Analysis (1958–2025)",
    "section": "",
    "text": "Code\nfrom bs4 import BeautifulSoup\nimport requests\nimport pandas as pd\nimport re \nimport random\nfrom time import sleep\n\n\n\n\nCode\n#first we need the master list of all the pages\nurl = \"https://en.wikipedia.org/wiki/Lists_of_Billboard_Hot_100_top-ten_singles\"\nheaders = {\"User-Agent\": \"Mozilla/5.0\"}  # Important to avoid HTTP 403\nresponse = requests.get(url, headers=headers)\nresponse.raise_for_status()  # Ensure it succeeded\nsoup = BeautifulSoup(response.text, \"html.parser\")\n\n\n\n\nCode\n# Find all relevant Wikipedia URLs for Billboard Hot 100 top-ten singles lists\nlinks = soup.find_all('a', href=True)\nbillboard_links = [\n    \"https://en.wikipedia.org\" + link['href']\n    for link in links\n    if link['href'].startswith('/wiki/List_of_Billboard_Hot_100_top-ten_singles_in')\n]\nbillboard_links = list(set(billboard_links))\n\n# Display the found URLs\nfor url in billboard_links:\n    print(url)\n\n\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2015\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1986\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1962\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1970\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1996\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1979\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1997\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1972\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1994\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2002\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2017\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1984\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2009\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2019\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1981\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1987\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1992\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2025\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2012\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2018\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1980\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1959\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2022\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2004\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2003\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2008\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2023\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1968\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1967\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1969\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1988\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1975\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1985\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1963\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1965\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1978\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1991\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2013\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1999\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1971\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1998\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2000\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1989\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1977\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1961\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1974\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2010\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1966\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2021\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1993\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2006\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2007\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2020\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1960\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1982\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2016\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2014\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1976\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2005\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1983\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1973\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2011\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1990\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1958\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2024\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2001\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1964\nhttps://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1995\n\n\n\n\nCode\nprint(f\"We found {len(billboard_links)} URLs.\")\n\n\nWe found 68 URLs.\n\n\n\n\nCode\ntest = requests.get(billboard_links[0], headers=headers)\ntest.raise_for_status()  # Ensure it succeeded\nsoup_test = BeautifulSoup(test.text, \"html.parser\")\n\n\n\n\nCode\ndef data_scraper(urls):\n    headers = {\"User-Agent\": \"Mozilla/5.0\"}  # Avoid HTTP 403\n    all_data = []\n\n    # Final column names\n    column_names = [\n        'Top Ten Entry Date',\n        'Single Name',\n        'Artist(s)',\n        'Peak',\n        'Peak Date',\n        'Weeks in Top Ten',\n        'Ref',\n        'Year'\n    ]\n\n    for url in urls:\n        print(f\"Scraping URL: {url}\")\n        response = requests.get(url, headers=headers)\n        response.raise_for_status()\n\n        soup = BeautifulSoup(response.text, \"html.parser\")\n        # We only need the first table on each page\n        table = soup.find(\"table\", {\"class\": \"wikitable\"})\n\n        # Convert sentinel &lt;th colspan=\"7\"&gt; into &lt;td colspan=\"7\"&gt; so pandas keeps them\n        for sentinel in table.find_all(\"th\"):\n            if sentinel.get(\"colspan\"):  # only the multi-col sentinel headers\n                sentinel.name = \"td\"\n\n        # Extract default year from URL\n        year = int(url[-4:])\n\n        # Read table into DataFrame, but keep all rows (including sentinel rows)\n        df = pd.read_html(str(table), header=0)[0]\n\n        # Handle missing 'Ref' column\n        if len(df.columns) == 6: \n            df['Ref'] = None\n\n        # Add year column, initially filled with default\n        df['Year'] = pd.NA\n\n        # --- Sentinel logic ---\n        # Find sentinel rows like \"Singles from 2024\"\n        mask = df.apply(\n            lambda row: row.astype(str).str.contains(r\"Singles from \\d{4}\").any(),\n            axis=1\n        )\n        print(f\"Found {mask.sum()} sentinel rows in this table.\")\n        if mask.sum() == 0:\n            # If no sentinels, fill year column with default year\n            df[\"Year\"] = year\n        else:\n            # Extract year from sentinel rows\n            df.loc[mask, \"Year\"] = df.loc[mask].apply(\n                lambda row: int(re.search(r\"\\d{4}\", \" \".join(row.astype(str))).group()),\n                axis=1\n            )\n        print(df.head(6))\n\n        # Forward-fill the year column\n        df[\"Year\"] = df[\"Year\"].ffill()\n\n        # Drop the sentinel rows\n        df = df[~mask].reset_index(drop=True)\n\n        # Rename columns consistently\n        df.columns = column_names\n\n        # Add year to peak date since this will always takes the url's year\n        df['Peak Date'] = (\n            df['Peak Date']\n            .str.replace(r'\\(.*\\)', '', regex=True)    # remove anything in parentheses\n            .str.replace(r'\\[\\d+\\]', '', regex=True)   # remove citation brackets like [1]\n            .str.strip()                               # remove leading/trailing whitespace\n        )\n        df['Peak Date'] = pd.to_datetime(df['Peak Date'].astype(str) + ' ' + str(year), errors='coerce')\n\n        # add year to entry date\n        df['Top Ten Entry Date'] = (\n            df['Top Ten Entry Date']\n            .str.replace(r'\\(.*\\)', '', regex=True)    # remove anything in parentheses\n            .str.replace(r'\\[\\d+\\]', '', regex=True)   # remove citation brackets like [1]\n            .str.strip()                               # remove leading/trailing whitespace\n        )\n        df['Top Ten Entry Date'] = pd.to_datetime(df['Top Ten Entry Date'].astype(str) + ' ' + df['Year'].astype(str), errors='coerce')\n\n        ### lets clean the data up a bit\n        #remove everything after the second \" in the single name\n        df['Single Name'] = df['Single Name'].str.split('\"').str[1]\n        #convert weeks in top ten to int\n        df['Weeks in Top Ten'] = df['Weeks in Top Ten'].astype(str).str.extract(r'(\\d+)')  # extract the number\n        df['Weeks in Top Ten'] = df['Weeks in Top Ten'].astype(int)\n        #convert peak to int\n        # Keep only digits and convert to integer\n        df['Peak'] = df['Peak'].astype(str).str.extract(r'(\\d+)')  # extract the number\n        df['Peak'] = df['Peak'].astype(int)  # convert to integer\n\n        all_data.append(df)\n        sleep(random.uniform(1, 3))  # sleep between 1 and 3 seconds\n\n    return pd.concat(all_data, ignore_index=True) if all_data else pd.DataFrame()\n\n\n\n\nCode\ndf = data_scraper(billboard_links[:5])\n\n\nScraping URL: https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_2025\n\n\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:35: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table), header=0)[0]\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:63: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"Year\"] = df[\"Year\"].ffill()\n\n\nFound 2 sentinel rows in this table.\n  Top ten entry date                      Single                 Artist(s)  \\\n0  Singles from 2024           Singles from 2024         Singles from 2024   \n1          August 31  \"Die with a Smile\"[B][P] ↑  Lady Gaga and Bruno Mars   \n2         November 2              \"Apt.\"[B][H] ↑       Rosé and Bruno Mars   \n3         December 7            \"Luther\"[B][O] ↑    Kendrick Lamar and SZA   \n4  Singles from 2025           Singles from 2025         Singles from 2025   \n\n                Peak          Peak date   Weeks in top ten               Ref.  \\\n0  Singles from 2024  Singles from 2024  Singles from 2024  Singles from 2024   \n1                  1         January 11                 50             [2][3]   \n2                  3         February 1                 14             [4][5]   \n3                  1            March 1                 32             [6][7]   \n4  Singles from 2025  Singles from 2025  Singles from 2025  Singles from 2025   \n\n   Year  \n0  2024  \n1  &lt;NA&gt;  \n2  &lt;NA&gt;  \n3  &lt;NA&gt;  \n4  2025  \nScraping URL: https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1958\nFound 0 sentinel rows in this table.\n  Top ten entry date               Single      Artist(s)  Peak Peak date  \\\n0        August 4[1]   \"Poor Little Fool\"   Ricky Nelson     1  August 4   \n1        August 4[1]           \"Patricia\"    Pérez Prado     2  August 4   \n2        August 4[1]      \"Splish Splash\"    Bobby Darin     3  August 4   \n3        August 4[1]  \"Hard Headed Woman\"  Elvis Presley     4  August 4   \n4        August 4[1]               \"When\"    Kalin Twins     5  August 4   \n\n   Weeks in top ten   Ref  Year  \n0                 6  None  1958  \n1                 6  None  1958  \n2                 3  None  1958  \n3                 2  None  1958  \n4                 5  None  1958  \n\n\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:35: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table), header=0)[0]\n\n\nScraping URL: https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1959\n\n\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:35: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table), header=0)[0]\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:63: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"Year\"] = df[\"Year\"].ffill()\n\n\nFound 2 sentinel rows in this table.\n  Top ten entry date                     Single          Artist(s)  \\\n0  Singles from 1958          Singles from 1958  Singles from 1958   \n1        December 15  \"Smoke Gets in Your Eyes\"       The Platters   \n2        December 22       \"A Lover's Question\"    Clyde McPhatter   \n3        December 29       \"Whole Lotta Lovin'\"        Fats Domino   \n4  Singles from 1959          Singles from 1959  Singles from 1959   \n\n                Peak          Peak date   Weeks in top ten   Ref  Year  \n0  Singles from 1958  Singles from 1958  Singles from 1958  None  1958  \n1                  1         January 19                 10  None  &lt;NA&gt;  \n2                  6         January 19                  7  None  &lt;NA&gt;  \n3                  6         January 12                  5  None  &lt;NA&gt;  \n4  Singles from 1959  Singles from 1959  Singles from 1959  None  1959  \nScraping URL: https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1960\n\n\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:35: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table), header=0)[0]\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:63: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"Year\"] = df[\"Year\"].ffill()\n\n\nFound 2 sentinel rows in this table.\n  Top ten entry date                            Single          Artist(s)  \\\n0  Singles from 1959                 Singles from 1959  Singles from 1959   \n1        December 21                         \"El Paso\"      Marty Robbins   \n2        December 21  \"Way Down Yonder in New Orleans\"      Freddy Cannon   \n3        December 28                \"Pretty Blue Eyes\"     Steve Lawrence   \n4  Singles from 1960                 Singles from 1960  Singles from 1960   \n\n                Peak          Peak date   Weeks in top ten   Ref  Year  \n0  Singles from 1959  Singles from 1959  Singles from 1959  None  1959  \n1                  1          January 4                  9  None  &lt;NA&gt;  \n2                  3         January 11                  7  None  &lt;NA&gt;  \n3                  9          January 4                  6  None  &lt;NA&gt;  \n4  Singles from 1960  Singles from 1960  Singles from 1960  None  1960  \nScraping URL: https://en.wikipedia.org/wiki/List_of_Billboard_Hot_100_top-ten_singles_in_1961\n\n\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:35: FutureWarning: Passing literal html to 'read_html' is deprecated and will be removed in a future version. To read from a literal string, wrap it in a 'StringIO' object.\n  df = pd.read_html(str(table), header=0)[0]\nC:\\Users\\harri\\AppData\\Local\\Temp\\ipykernel_109928\\1447691552.py:63: FutureWarning: Downcasting object dtype arrays on .fillna, .ffill, .bfill is deprecated and will change in a future version. Call result.infer_objects(copy=False) instead. To opt-in to the future behavior, set `pd.set_option('future.no_silent_downcasting', True)`\n  df[\"Year\"] = df[\"Year\"].ffill()\n\n\nFound 2 sentinel rows in this table.\n  Top ten entry date                 Single                Artist(s)  \\\n0  Singles from 1960      Singles from 1960        Singles from 1960   \n1        December 12  \"Wonderland by Night\"           Bert Kaempfert   \n2        December 12               \"Exodus\"       Ferrante & Teicher   \n3        December 26     \"Corrina, Corinna\"             Ray Peterson   \n4     December 31[1]           \"Angel Baby\"  Rosie and the Originals   \n\n                Peak          Peak date   Weeks in top ten   Ref  Year  \n0  Singles from 1960  Singles from 1960  Singles from 1960  None  1960  \n1                  1          January 9                 10  None  &lt;NA&gt;  \n2                  2         January 23                 11  None  &lt;NA&gt;  \n3                  9          January 9                  5  None  &lt;NA&gt;  \n4                  5         January 23                  7  None  &lt;NA&gt;  \n\n\n\n\nCode\nprint(len(df))\nprint(df['Year'].unique())\nprint(df['Year'].value_counts())\n\n\n365\n[2024 2025 1958 1959 1960 1961]\nYear\n1961    101\n1960     95\n1959     91\n1958     43\n2025     32\n2024      3\nName: count, dtype: int64\n\n\n\n\nCode\ndf[df['Year'] == 1958].head(20)\n\n\n\n\n\n\n\n\n\nTop Ten Entry Date\nSingle Name\nArtist(s)\nPeak\nPeak Date\nWeeks in Top Ten\nRef\nYear\n\n\n\n\n35\n1958-08-04\nPoor Little Fool\nRicky Nelson\n1\n1958-08-04\n6\nNone\n1958\n\n\n36\n1958-08-04\nPatricia\nPérez Prado\n2\n1958-08-04\n6\nNone\n1958\n\n\n37\n1958-08-04\nSplish Splash\nBobby Darin\n3\n1958-08-04\n3\nNone\n1958\n\n\n38\n1958-08-04\nHard Headed Woman\nElvis Presley\n4\n1958-08-04\n2\nNone\n1958\n\n\n39\n1958-08-04\nWhen\nKalin Twins\n5\n1958-08-04\n5\nNone\n1958\n\n\n40\n1958-08-04\nRebel 'Rouser\nDuane Eddy\n6\n1958-08-04\n3\nNone\n1958\n\n\n41\n1958-08-04\nYakety Yak\nThe Coasters\n7\n1958-08-04\n1\nNone\n1958\n\n\n42\n1958-08-04\nMy True Love\nJack Scott\n3\n1958-08-18\n6\nNone\n1958\n\n\n43\n1958-08-04\nWillie and the Hand Jive\nThe Johnny Otis Show\n9\n1958-08-04\n2\nNone\n1958\n\n\n44\n1958-08-04\nFever\nPeggy Lee\n8\n1958-08-25\n3\nNone\n1958\n\n\n45\n1958-08-11\nNel Blu Dipinto Di Blu (Volare)\nDomenico Modugno\n1\n1958-08-18\n10\nNone\n1958\n\n\n46\n1958-08-11\nJust a Dream\nJimmy Clanton\n4\n1958-08-25\n8\nNone\n1958\n\n\n47\n1958-08-18\nLittle Star\nThe Elegants\n1\n1958-08-25\n9\nNone\n1958\n\n\n48\n1958-08-25\nBird Dog\nThe Everly Brothers\n2\n1958-09-15\n11\nNone\n1958\n\n\n49\n1958-08-25\nBorn Too Late\nThe Poni-Tails\n7\n1958-09-15\n3\nNone\n1958\n\n\n50\n1958-09-01\nGinger Bread\nFrankie Avalon\n9\n1958-09-01\n2\nNone\n1958\n\n\n51\n1958-09-01\nAre You Really Mine?\nJimmie Rodgers\n10\n1958-09-01\n1\nNone\n1958\n\n\n52\n1958-09-08\nRockin' Robin\nBobby Day\n2\n1958-10-13\n10\nNone\n1958\n\n\n53\n1958-09-08\nWestern Movies\nThe Olympics\n8\n1958-09-15\n2\nNone\n1958\n\n\n54\n1958-09-15\nIt's All in the Game\nTommy Edwards\n1\n1958-09-29\n12\nNone\n1958\n\n\n\n\n\n\n\n\n\nCode\ndf\n\n\n\n\n\n\n\n\n\nTop Ten Entry Date\nSingle Name\nArtist(s)\nPeak\nPeak Date\nWeeks in Top Ten\nRef\nYear\n\n\n\n\n0\n2024-08-31\nDie with a Smile\nLady Gaga and Bruno Mars\n1\n2025-01-11\n50\n[2][3]\n2024\n\n\n1\n2024-11-02\nApt.\nRosé and Bruno Mars\n3\n2025-02-01\n14\n[4][5]\n2024\n\n\n2\n2024-12-07\nLuther\nKendrick Lamar and SZA\n1\n2025-03-01\n32\n[6][7]\n2024\n\n\n3\n2025-01-18\nSmile\nMorgan Wallen\n4\n2025-01-18\n1\n[8]\n2025\n\n\n4\n2025-01-25\nDTMF\nBad Bunny\n2\n2025-01-25\n3\n[9]\n2025\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n360\n1961-12-04\nI Understand (Just How You Feel)\nThe G-Clefs\n9\n1961-12-04\n1\nNone\n1961\n\n\n361\n1961-12-11\nThe Lion Sleeps Tonight\nThe Tokens\n1\n1961-12-18\n8\nNone\n1961\n\n\n362\n1961-12-11\nRun to Him\nBobby Vee\n2\n1961-12-25\n6\nNone\n1961\n\n\n363\n1961-12-11\nTonight\nFerrante & Teicher\n8\n1961-12-11\n1\nNone\n1961\n\n\n364\n1961-12-11\nLet There Be Drums\nSandy Nelson\n7\n1961-12-18\n3\nNone\n1961\n\n\n\n\n365 rows × 8 columns"
  },
  {
    "objectID": "notebooks/analysis.html",
    "href": "notebooks/analysis.html",
    "title": "Billboard Top Ten Analysis (1958–2025)",
    "section": "",
    "text": "Code\nimport re\nfrom pathlib import Path\nimport pandas as pd\nimport numpy as np\nimport plotly.express as px\nimport plotly.graph_objects as go\nimport plotly.figure_factory as ff\nfrom lifelines import KaplanMeierFitter\nfrom scipy.cluster.hierarchy import linkage, dendrogram\nfrom scipy.stats import chi2_contingency, ttest_ind, f_oneway, kruskal\nfrom sklearn.cluster import KMeans\nfrom scipy.cluster.hierarchy import dendrogram, linkage\nimport networkx as nx\nimport calendar\n\n\n# Get the path to the repo root (assuming notebook is in root or subfolder)\nnotebook_dir = Path().resolve()  # current working directory\n# Go up one level to repo root\nrepo_root = notebook_dir.parent\n\n\n\n\nCode\n# Load data\n# Path to CSV\ncsv_path = repo_root / \"data\" / \"billboard_data_2025_09.csv\"\n\n# Load the CSV\ndf = pd.read_csv(csv_path)\ndf.head()\n\n\n\n\n\n\n\n\n\nTop Ten Entry Date\nSingle Name\nArtist(s)\nPeak\nPeak Date\nWeeks in Top Ten\nRef\nYear\n\n\n\n\n0\n1960-12-12\nWonderland by Night\nBert Kaempfert\n1\n1961-01-09\n10\nNaN\n1960\n\n\n1\n1960-12-12\nExodus\nFerrante & Teicher\n2\n1961-01-23\n11\nNaN\n1960\n\n\n2\n1960-12-26\nCorrina, Corinna\nRay Peterson\n9\n1961-01-09\n5\nNaN\n1960\n\n\n3\n1960-12-31\nAngel Baby\nRosie and the Originals\n5\n1961-01-23\n7\nNaN\n1960\n\n\n4\n1961-01-09\nWill You Love Me Tomorrow\nThe Shirelles\n1\n1961-01-30\n7\nNaN\n1961\n\n\n\n\n\n\n\n\n\nCode\n#some preprocessing\ndf['Top Ten Entry Date'] = pd.to_datetime(df['Top Ten Entry Date'], errors='coerce')\ndf['Peak Date'] = pd.to_datetime(df['Peak Date'], errors='coerce')\n# Create a decade column\ndf['Decade'] = (df['Top Ten Entry Date'].dt.year // 10) * 10\n\n\n\n\nCode\n# Questions I want answers to:\n# 1. What are the top 10 genres with the most number of songs in the dataset?\n# 2. Average weeks in top ten by decade\n# 3. Average weeks in top ten for peak 1 positions vs peak 10 positions\n# 4. item 3 but by decade \n\n\n\n\nCode\nprint(f\"The overall average number of a weeks a song spends in the top 10 is {df['Weeks in Top Ten'].mean()} weeks\")\nfig = px.bar(\n    df['Weeks in Top Ten'].value_counts().sort_index().reset_index(),\n    x='Weeks in Top Ten',\n    y='count',\n    text_auto=True,\n    title=\"Overall Distribution of Weeks in Top Ten (Exact Weeks)\"\n)\n\nfig.update_layout(\n    xaxis_title=\"Weeks in Top Ten\",\n    yaxis_title=\"Number of Songs\"\n)\n\nfig.show()\n\n# Aggregate: average weeks in Top Ten by decade\ndecade_stats = df.groupby('Decade', as_index=False)['Weeks in Top Ten'].mean()\n\n# Plot with Plotly\nfig = px.bar(\n    decade_stats,\n    x='Decade',\n    y='Weeks in Top Ten',\n    text_auto='.2f',\n    title=\"Average Weeks in Top Ten by Decade\"\n)\n\n# Make labels clearer\nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\nfig.update_layout(\n    xaxis_title=\"Decade\",\n    yaxis_title=\"Average Weeks in Top Ten\"\n)\n\nfig.show()\n\n\n\n#Per Year\ntrend = df.groupby(\"Year\")[\"Weeks in Top Ten\"].mean().reset_index()\n\n# Plot with Plotly Express\nfig = px.line(\n    trend,\n    x=\"Year\",\n    y=\"Weeks in Top Ten\",\n    title=\"Average Weeks in Top Ten Over Time Per Year\",\n    labels={\"Weeks in Top Ten\": \"Avg Weeks\", \"Year\": \"Year\"}\n)\n\nfig.show()\n\n\nThe overall average number of a weeks a song spends in the top 10 is 6.641318848866018 weeks\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Distribution of Peak Positions\nfig = px.bar(\n    df['Peak'].value_counts().sort_index().reset_index(),\n    x='Peak',\n    y='count',\n    text_auto=True,\n    title=\"Overall Distribution of Peak Positions\"\n)\n\nfig.update_layout(\n    xaxis_title=\"Peak Position\",\n    yaxis_title=\"Number of Songs\",\n    xaxis=dict(tickmode='linear', dtick=1)  # &lt;- force every tick to show\n)\n\nfig.show()\n\n# Aggregate: average weeks in Top Ten by decade\ndecade_stats = df.groupby('Decade', as_index=False)['Peak'].mean()\n\n# Plot with Plotly\nfig = px.bar(\n    decade_stats,\n    x='Decade',\n    y='Peak',\n    text_auto='.2f',\n    title=\"Average Peak Position in Top Ten by Decade\"\n)\n\n# Make labels clearer\nfig.update_traces(textfont_size=12, textangle=0, textposition=\"outside\", cliponaxis=False)\nfig.update_layout(\n    xaxis_title=\"Decade\",\n    yaxis_title=\"Average Peak Position in Top Ten\"\n)\n\nfig.show()\n\n\n#Peak Position Per Year\ntrend = df.groupby(\"Year\")[\"Peak\"].mean().reset_index()\n\n# Plot with Plotly Express\nfig = px.line(\n    trend,\n    x=\"Year\",\n    y=\"Peak\",\n    title=\"Average Peak Position in Top Ten Over Time Per Year\",\n    labels={\"Peak\": \"Avg Peak Position\", \"Year\": \"Year\"}\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Fit KM survival curve\nkmf = KaplanMeierFitter()\nkmf.fit(df[\"Weeks in Top Ten\"], event_observed=[1]*len(df))  # all songs eventually \"fail\"\n\n# Get survival function as dataframe\nsurvival_df = kmf.survival_function_.reset_index()\nsurvival_df.columns = [\"Weeks in Top Ten\", \"Probability of Staying\"]\n\n# Plot with Plotly\nfig = px.line(\n    survival_df,\n    x=\"Weeks in Top Ten\",\n    y=\"Probability of Staying\",\n    title=\"Survival Curve of Billboard Top Ten Singles\",\n    labels={\n        \"Weeks in Top Ten\": \"Weeks in Top Ten\",\n        \"Probability of Staying\": \"Probability of Staying in Top Ten\"\n    }\n)\n\n# Style adjustments\nfig.update_traces(mode=\"lines+markers\")\nfig.update_layout(yaxis=dict(range=[0, 1]))\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n#Survival analysis by decade\n# Clean figure\nfig = go.Figure()\nkmf = KaplanMeierFitter()\n\n# Only unique decades\ndecades = sorted(df['Decade'].dropna().unique())\n\nfor i, decade in enumerate(decades):\n    group = df[df['Decade'] == decade]\n    if len(group) == 0:\n        continue\n\n    # Fit Kaplan-Meier curve\n    kmf.fit(group[\"Weeks in Top Ten\"], event_observed=[1]*len(group))\n    surv = kmf.survival_function_.reset_index()\n    surv.columns = [\"Weeks in Top Ten\", \"Probability of Staying\"]\n\n    # Add survival curve\n    fig.add_trace(\n        go.Scatter(\n            x=surv[\"Weeks in Top Ten\"],\n            y=surv[\"Probability of Staying\"],\n            mode=\"lines\",\n            name=str(decade),\n            line=dict(width=3)\n        )\n    )\n\n    # Automatically pick evenly spaced points along the middle 80% of the curve\n    n_labels = 1\n    start_idx = int(len(surv) * 0.3)   # skip first 10%\n    end_idx = int(len(surv) * 0.95)     # skip last 10%\n    indices = np.linspace(start_idx, end_idx, n_labels, dtype=int)\n\n    for j, idx in enumerate(indices):\n        # stagger labels vertically to reduce overlap\n        y_offset = 15 * (i - len(decades)/2) + 10 * (j - (n_labels-1)/2)\n\n        # Add annotation once per point\n        fig.add_annotation(\n            x=surv[\"Weeks in Top Ten\"].iloc[idx],\n            y=surv[\"Probability of Staying\"].iloc[idx],\n            text=f\"{decade}\",\n            showarrow=True,\n            arrowhead=2,\n            font=dict(size=12),\n            ax=0,\n            ay=-y_offset\n        )\n\n# Layout\nfig.update_layout(\n    title=\"Survival Curves of Billboard Top Ten Singles by Decade\",\n    xaxis_title=\"Weeks in Top Ten\",\n    yaxis_title=\"Probability of Staying in Top Ten\",\n    yaxis=dict(range=[0, 1]),\n    showlegend=False\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Clean figure\nfig = go.Figure()\nkmf = KaplanMeierFitter()\n\n# Only unique decades\ndecades = sorted(df['Decade'].dropna().unique())\n\nfor i, decade in enumerate(decades):\n    group = df[df['Decade'] == decade]\n    if len(group) == 0:\n        continue\n\n    # Fit Kaplan-Meier curve\n    kmf.fit(group[\"Weeks in Top Ten\"], event_observed=[1]*len(group))\n    surv = kmf.survival_function_.reset_index()\n    surv.columns = [\"Weeks in Top Ten\", \"Probability of Staying\"]\n\n    # Add survival curve\n    fig.add_trace(\n        go.Scatter(\n            x=surv[\"Weeks in Top Ten\"],\n            y=surv[\"Probability of Staying\"],\n            mode=\"lines\",\n            name=str(decade),\n            line=dict(width=3)\n        )\n    )\n\n    # Place label at 80% of curve\n    idx = int(len(surv) * 0.45)\n\n    # stagger labels vertically\n    y_offset = 15 * (i - len(decades)/2)\n\n    # Add annotation with custom font\n    fig.add_annotation(\n        x=surv[\"Weeks in Top Ten\"].iloc[idx],\n        y=surv[\"Probability of Staying\"].iloc[idx],\n        text=f\"{decade}\",\n        showarrow=True,\n        arrowhead=3,\n        font=dict(family=\"Arial\", size=13, color=\"black\"),  # base font\n        xanchor=\"left\",\n        yanchor=\"middle\"\n    )\n\n# Layout\nfig.update_layout(\n    title=\"Survival Curves of Billboard Top Ten Singles by Decade\",\n    xaxis_title=\"Weeks in Top Ten\",\n    yaxis_title=\"Probability of Staying in Top Ten\",\n    yaxis=dict(range=[0, 1], tickvals=np.arange(0.1, 1.1, 0.1), tickformat=\".0%\"),  # remove 0\n    showlegend=False\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Average weeks in Top Ten by peak position\nlongevity_by_peak = df.groupby(\"Peak\")[\"Weeks in Top Ten\"].mean().reset_index()\n\n# Sort by peak position\nlongevity_by_peak = longevity_by_peak.sort_values(\"Peak\")\n\n# Plot with Plotly\nfig = px.bar(\n    longevity_by_peak,\n    x=\"Peak\",\n    y=\"Weeks in Top Ten\",\n    text=\"Weeks in Top Ten\",\n    labels={\"Peak\": \"Peak Position\", \"Weeks in Top Ten\": \"Avg Weeks\"},\n    title=\"Average Weeks in Top Ten by Peak Position\"\n)\n\n# Show exact values on bars\nfig.update_traces(texttemplate=\"%{text:.2f}\", textposition=\"outside\")\n\n# Optional: style layout\nfig.update_layout(\n    yaxis_title=\"Avg Weeks\",\n    xaxis_title=\"Peak Position\",\n    uniformtext_minsize=8,\n    uniformtext_mode='hide'\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Number of entries per year\nentries_per_year = df.groupby(\"Year\")[\"Single Name\"].count().reset_index()\nentries_per_year.rename(columns={\"Single Name\": \"Count\"}, inplace=True)\n\n# Plot with Plotly Express\nfig = px.line(\n    entries_per_year,\n    x=\"Year\",\n    y=\"Count\",\n    title=\"Number of Top Ten Entries per Year\",\n    labels={\"Year\": \"Year\", \"Count\": \"Number of Entries\"}\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Top 10 artists by number of Top Ten entries\ntop_artists = df[\"Artist(s)\"].value_counts().head(10).reset_index()\ntop_artists.columns = [\"Artist\", \"Entries\"]\n\n# Plotly horizontal bar chart\nfig = px.bar(\n    top_artists,\n    x=\"Entries\",\n    y=\"Artist\",\n    orientation=\"h\",\n    text=\"Entries\",\n    title=\"Top Artists by Top Ten Entries\"\n)\n\n# Show exact numbers on bars\nfig.update_traces(texttemplate=\"%{text}\", textposition=\"outside\")\n\n# Optional layout adjustments\nfig.update_layout(\n    xaxis_title=\"Number of Entries\",\n    yaxis_title=\"Artist\",\n    yaxis=dict(autorange=\"reversed\")  # keep highest at top\n)\n\nfig.show()\n\n# One-hit wonders\nartist_song_counts = df.groupby(\"Artist(s)\")[\"Single Name\"].nunique()\none_hit_pct = (artist_song_counts == 1).mean() * 100\nprint(f\"One-hit wonders (Artists that reach the top 10 only once): {one_hit_pct:.1f}% of artists\")\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nOne-hit wonders (Artists that reach the top 10 only once): 70.2% of artists\n\n\n\n\nCode\n# Calculate total weeks in Top Ten for each artist\nartist_real_estate = df.groupby(\"Artist(s)\")[\"Weeks in Top Ten\"].sum().reset_index()\nartist_real_estate = artist_real_estate.sort_values(\"Weeks in Top Ten\", ascending=False).head(10)\n\n# Plot horizontal bar chart\nfig = px.bar(\n    artist_real_estate,\n    x=\"Weeks in Top Ten\",\n    y=\"Artist(s)\",\n    orientation=\"h\",\n    text=\"Weeks in Top Ten\",\n    title=\"Top 10 Artists by Total Weeks in Top Ten (Chart Real Estate)\"\n)\n\nfig.update_traces(textposition=\"outside\")\nfig.update_layout(\n    xaxis_title=\"Total Weeks in Top Ten\",\n    yaxis_title=\"Artist\",\n    yaxis=dict(autorange=\"reversed\")\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n#Seasonal Top Ten Entries by Month and Decade\n\n# Ensure Decade and Month columns exist\ndf['Decade'] = (df['Top Ten Entry Date'].dt.year // 10) * 10\ndf['Month'] = df['Top Ten Entry Date'].dt.month\n\n# Count entries by month and decade\nentries_by_month_decade = df.groupby(['Month', 'Decade'])['Single Name'].count().reset_index()\nentries_by_month_decade.rename(columns={\"Single Name\": \"Entries\"}, inplace=True)\n\n# Convert numeric months to abbreviated month names\nentries_by_month_decade['Month Name'] = entries_by_month_decade['Month'].apply(lambda x: calendar.month_abbr[x])\nfig=()\n# Plot grouped bar chart\nfig = px.bar(\n    entries_by_month_decade,\n    x='Month Name',\n    y='Entries',\n    color='Decade',\n    barmode='group',  # grouped bars\n    text='Entries',\n    title='Top Ten Entries by Month and Decade',\n    height=700\n)\n\n# Show numerical labels inside bars\nfig.update_traces(textposition='inside')\n\n# Improve layout\nfig.update_layout(\n    xaxis_title=\"Month\",\n    yaxis_title=\"Number of Entries\",\n    uniformtext_minsize=8,\n    uniformtext_mode='hide'\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n#Song Trajectory (Entry - Peak Lag)\n\n# Compute Lag to Peak\ndf[\"Lag to Peak\"] = (df[\"Peak Date\"] - df[\"Top Ten Entry Date\"]).dt.days\n\n# Drop NaNs for plotting\nlag_data = df[\"Lag to Peak\"].dropna()\n\n# Create histogram\nfig = px.histogram(\n    lag_data,\n    nbins=30,\n    title=\"Distribution of Days to Peak Position\",\n    labels={\"value\": \"Days\", \"count\": \"Count\"}\n)\n\n# Optional: show counts on hover\nfig.update_traces(marker_color='blue', opacity=0.8)\n\n# Adjust layout\nfig.update_layout(\n    xaxis_title=\"Days\",\n    yaxis_title=\"Count\",\n    bargap=0.05,\n    height=600\n)\n\nfig.show()\n\n\n\n# Get top 10 songs with greatest lag\ntop_lag = df.nlargest(10, \"Lag to Peak\")[[\"Single Name\", \"Artist(s)\", \"Lag to Peak\", \"Top Ten Entry Date\", \"Peak Date\"]]\ntop_lag['Label'] = top_lag[\"Single Name\"] + \" – \" + top_lag[\"Artist(s)\"]\n\n# Horizontal bar chart with hover info\nfig_top_lag = px.bar(\n    top_lag,\n    x=\"Lag to Peak\",\n    y=\"Label\",\n    orientation='h',\n    text=\"Lag to Peak\",\n    hover_data={\n        \"Lag to Peak\": True,\n        \"Top Ten Entry Date\": True,\n        \"Peak Date\": True,\n        \"Label\": False  # hide duplicate label in hover\n    },\n    title=\"Top 10 Songs with Greatest Lag to Peak\"\n)\n\n# Show numbers outside bars\nfig_top_lag.update_traces(textposition=\"outside\")\n\n# Keep the largest lag at top\nfig_top_lag.update_layout(\n    yaxis=dict(autorange=\"reversed\"),\n    height=800,\n    xaxis_title=\"Days to Peak\",\n    yaxis_title=\"\"\n)\n\nfig_top_lag.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n\n# Longest-running songs\nlongest = df.nlargest(20, \"Weeks in Top Ten\")[[\"Single Name\", \"Artist(s)\", \"Weeks in Top Ten\"]]\nlongest['Label'] = longest[\"Single Name\"] + \" – \" + longest[\"Artist(s)\"]\n\nfig_longest = px.bar(\n    longest,\n    x=\"Weeks in Top Ten\",\n    y=\"Label\",\n    orientation='h',\n    text=\"Weeks in Top Ten\",\n    title=\"Top 20 Longest-Running Top Ten Songs\"\n)\n\nfig_longest.update_traces(textposition=\"outside\")\nfig_longest.update_layout(\n    height=800,\n    yaxis=dict(\n        automargin=True,\n        autorange=\"reversed\"))  # longest at top\nfig_longest.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\ndef clustering_plotly(df, k=4):\n    print(\"=== Clustering Song Trajectories ===\")\n    \n    # Prepare features\n    features = df[[\"Lag to Peak\", \"Weeks in Top Ten\", \"Peak\"]].dropna()\n    \n    # K-means clustering\n    km = KMeans(n_clusters=k, random_state=42, n_init=10).fit(features)\n    df.loc[features.index, \"Cluster\"] = km.labels_.astype(str)  # convert to string for coloring\n    \n    # Scatter plot of K-means clusters\n    fig_scatter = px.scatter(\n        df.loc[features.index],\n        x=\"Lag to Peak\",\n        y=\"Weeks in Top Ten\",\n        color=\"Cluster\",\n        hover_data=[\"Peak\", \"Lag to Peak\", \"Weeks in Top Ten\", \"Single Name\", \"Artist(s)\"],\n        title=\"K-means Clustering of Songs\",\n        height=600\n    )\n    fig_scatter.show()\n    \n    # Hierarchical clustering (subset for performance)\n    subset = features.sample(min(150, len(features)), random_state=42)\n    Z = linkage(subset, method=\"ward\")\n    \n    # Convert linkage to dendrogram in Plotly\n    # fig_dendro = ff.create_dendrogram(subset, orientation='top', labels=subset.index.astype(str))\n    # fig_dendro.update_layout(width=1000, height=500, title=\"Hierarchical Clustering Dendrogram (subset)\")\n    # fig_dendro.show()\n\nclustering_plotly(df, k=4)\n\n\n=== Clustering Song Trajectories ===\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n# Looks like my sleeper hits (long lag but long weeks in top ten) are:\ncluster_2_songs = df[df[\"Cluster\"] == \"2\"][[\"Single Name\", \"Artist(s)\", \"Weeks in Top Ten\", \"Lag to Peak\", \"Peak\"]]\ncluster_2_songs_sorted = cluster_2_songs.sort_values(by=[\"Peak\", \"Weeks in Top Ten\"], ascending=[True, False])\n\n# Create label combining song and artist\ncluster_2_songs_sorted['Label'] = cluster_2_songs_sorted[\"Single Name\"] + \" – \" + cluster_2_songs_sorted[\"Artist(s)\"]\n\n# Horizontal bar chart\nfig_cluster2 = px.bar(\n    cluster_2_songs_sorted,\n    x=\"Weeks in Top Ten\",\n    y=\"Label\",\n    orientation='h',\n    text=\"Weeks in Top Ten\",\n    hover_data=[\"Peak\", \"Lag to Peak\"],\n    title=\"Sleeper Hits (Songs that Peaked Late but Stayed Long)\",\n    height=800\n)\n\n# Show numbers outside bars\nfig_cluster2.update_traces(textposition=\"outside\")\n\n# Keep the highest peaks at the top\nfig_cluster2.update_layout(\n    yaxis=dict(autorange=\"reversed\", automargin=True),\n    xaxis_title=\"Weeks in Top Ten\",\n    yaxis_title=\"Single – Artist\",\n    height = 1000\n)\n\nfig_cluster2.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json\n\n\n\n\nCode\n#Artist Collaboration Networks by Decade\nframes = []\ndecade_list = sorted(df['Decade'].dropna().unique())\n\n# Prepare a graph for each decade\nfor decade in decade_list:\n    group = df[df['Decade'] == decade]\n    G = nx.Graph()\n    for artists in group[\"Artist(s)\"].dropna():\n        cleaned = re.sub(\n            r'\\s*(?:&|\\bfeat\\.?\\b|\\bfeaturing\\b|,|\\band\\b|\\&)\\s*',\n            ',',\n            str(artists),\n            flags=re.IGNORECASE\n        )\n        names = [a.strip() for a in cleaned.split(',') if a.strip()]\n        for i in range(len(names)):\n            for j in range(i+1, len(names)):\n                G.add_edge(names[i], names[j])\n\n    # Degree centrality\n    deg = nx.degree_centrality(G)\n    top_artists = sorted(deg.items(), key=lambda x: x[1], reverse=True)[:10]\n    subG = G.subgraph(dict(top_artists).keys())\n    pos = nx.spring_layout(subG, seed=42)\n\n    # Edge traces\n    edge_x, edge_y = [], []\n    for edge in subG.edges():\n        x0, y0 = pos[edge[0]]\n        x1, y1 = pos[edge[1]]\n        edge_x += [x0, x1, None]\n        edge_y += [y0, y1, None]\n\n    edge_trace = go.Scatter(\n        x=edge_x, y=edge_y,\n        line=dict(width=1, color='#888'),\n        hoverinfo='none',\n        mode='lines'\n    )\n\n    # Node traces\n    node_x, node_y, node_text, node_size = [], [], [], []\n    for node in subG.nodes():\n        x, y = pos[node]\n        node_x.append(x)\n        node_y.append(y)\n        node_text.append(node)\n        node_size.append(deg[node]*2000)\n\n    node_trace = go.Scatter(\n        x=node_x, y=node_y,\n        mode='markers+text',\n        hovertext=node_text,\n        text=node_text,\n        textposition='top center',\n        marker=dict(size=node_size, color='lightblue'),\n        showlegend=False\n    )\n\n    frames.append(go.Frame(data=[edge_trace, node_trace], name=str(decade)))\n\n# Slider steps\nslider_steps = [\n    dict(method=\"animate\",\n         args=[[str(decade)],\n               {\"frame\": {\"duration\": 0, \"redraw\": True},\n                \"mode\": \"immediate\"}],\n         label=str(decade))\n    for decade in decade_list\n]\n\n# Initial figure\nfig = go.Figure(\n    data=frames[0].data,\n    layout=go.Layout(\n        title=\"Top Artist Collaboration Network by Decade\",\n        width=1000,   # increase figure width\n        height=1000,   # increase figure height\n        xaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        yaxis=dict(showgrid=False, zeroline=False, showticklabels=False),\n        showlegend=False,\n        sliders=[dict(\n            active=0,\n            currentvalue={\"prefix\": \"Decade: \"},\n            pad={\"t\": 50},\n            steps=slider_steps\n        )]\n    ),\n    frames=frames\n)\n\nfig.show()\n\n\nUnable to display output for mime type(s): application/vnd.plotly.v1+json"
  },
  {
    "objectID": "notebooks/spotify_api_test.html",
    "href": "notebooks/spotify_api_test.html",
    "title": "Billboard Top Ten Analysis (1958–2025)",
    "section": "",
    "text": "Code\nimport requests\n\nfrom pathlib import Path\nimport pandas as pd\n# Get the path to the repo root (assuming notebook is in root or subfolder)\nnotebook_dir = Path().resolve()  # current working directory\n# Go up one level to repo root\nrepo_root = notebook_dir.parent\n\nurl = \"https://accounts.spotify.com/api/token\"\n\npayload = {\n    \"grant_type\": \"client_credentials\",\n    \"client_id\": \"XXXXXXXXXX\",\n    \"client_secret\": \"XXXXXXXXXX\"\n}\n\nheaders = {\n    \"Content-Type\": \"application/x-www-form-urlencoded\"\n}\n\n\n\n\nCode\n# Load data\n# Path to CSV\ncsv_path = repo_root / \"data\" / \"billboard_data_2025_09.csv\"\n\n# Load the CSV\ndf = pd.read_csv(csv_path)\ndf.head()\n\n\n\n\n\n\n\n\n\nTop Ten Entry Date\nSingle Name\nArtist(s)\nPeak\nPeak Date\nWeeks in Top Ten\nRef\nYear\n\n\n\n\n0\n1960-12-12\nWonderland by Night\nBert Kaempfert\n1\n1961-01-09\n10\nNaN\n1960\n\n\n1\n1960-12-12\nExodus\nFerrante & Teicher\n2\n1961-01-23\n11\nNaN\n1960\n\n\n2\n1960-12-26\nCorrina, Corinna\nRay Peterson\n9\n1961-01-09\n5\nNaN\n1960\n\n\n3\n1960-12-31\nAngel Baby\nRosie and the Originals\n5\n1961-01-23\n7\nNaN\n1960\n\n\n4\n1961-01-09\nWill You Love Me Tomorrow\nThe Shirelles\n1\n1961-01-30\n7\nNaN\n1961\n\n\n\n\n\n\n\n\n\nCode\nresponse = requests.post(url, data=payload, headers=headers)\n\nif response.status_code == 200:\n    token_info = response.json()\n    access_token = token_info[\"access_token\"]\n    print(\"Access token:\", \"redacted_for_security\")\nelse:\n    print(\"Error:\", response.status_code, response.text)\n\n\nAccess token: redacted_for_security\n\n\n\n\nCode\n#Search for the track by title + artist\ndef search_track(song_title, artist_name):\n    search_url = \"https://api.spotify.com/v1/search\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    query = f\"track:{song_title} artist:{artist_name}\"\n    params = {\"q\": query, \"type\": \"track\", \"limit\": 1}\n\n    response = requests.get(search_url, headers=headers, params=params)\n    print(response.json())\n    items = response.json().get(\"tracks\", {}).get(\"items\", [])\n    \n    if items:\n        track = items[0]\n        track_name = track[\"name\"]\n        artist_id = track[\"artists\"][0][\"id\"]\n        return track_name, artist_id\n    else:\n        return None, None\n\n# Step 3: Get artist genres\ndef get_artist_genres(artist_id):\n    artist_url = f\"https://api.spotify.com/v1/artists/{artist_id}\"\n    headers = {\"Authorization\": f\"Bearer {access_token}\"}\n    response = requests.get(artist_url, headers=headers)\n    print(response.json())\n    return response.json().get(\"genres\", [])\n\n# Example usage\nsong = \"Jolene\"\nartist = \"Beyonce\"\n\ntrack_name, artist_id = search_track(song, artist)\nif artist_id:\n    genres = get_artist_genres(artist_id)\n    print(f\"Song: {track_name}\")\n    print(f\"Artist genres: {genres}\")\nelse:\n    print(\"Track not found\")\n\n\n{'tracks': {'href': 'https://api.spotify.com/v1/search?offset=0&limit=1&query=track%3AJolene%20artist%3ABeyonce&type=track', 'limit': 1, 'next': 'https://api.spotify.com/v1/search?offset=1&limit=1&query=track%3AJolene%20artist%3ABeyonce&type=track', 'offset': 0, 'previous': None, 'total': 2, 'items': [{'album': {'album_type': 'album', 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/6vWDO969PvNqNYHIOW5v0m'}, 'href': 'https://api.spotify.com/v1/artists/6vWDO969PvNqNYHIOW5v0m', 'id': '6vWDO969PvNqNYHIOW5v0m', 'name': 'Beyoncé', 'type': 'artist', 'uri': 'spotify:artist:6vWDO969PvNqNYHIOW5v0m'}], 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BT', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GE', 'GD', 'GW', 'GY', 'HT', 'JM', 'KI', 'LS', 'LR', 'MW', 'MV', 'ML', 'MH', 'FM', 'NA', 'NR', 'NE', 'PW', 'PG', 'PR', 'WS', 'SM', 'ST', 'SN', 'SC', 'SL', 'SB', 'KN', 'LC', 'VC', 'SR', 'TL', 'TO', 'TT', 'TV', 'VU', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'external_urls': {'spotify': 'https://open.spotify.com/album/6BzxX6zkDsYKFJ04ziU5xQ'}, 'href': 'https://api.spotify.com/v1/albums/6BzxX6zkDsYKFJ04ziU5xQ', 'id': '6BzxX6zkDsYKFJ04ziU5xQ', 'images': [{'height': 640, 'width': 640, 'url': 'https://i.scdn.co/image/ab67616d0000b2731572698fff8a1db257a53599'}, {'height': 300, 'width': 300, 'url': 'https://i.scdn.co/image/ab67616d00001e021572698fff8a1db257a53599'}, {'height': 64, 'width': 64, 'url': 'https://i.scdn.co/image/ab67616d000048511572698fff8a1db257a53599'}], 'is_playable': True, 'name': 'COWBOY CARTER', 'release_date': '2024-03-29', 'release_date_precision': 'day', 'total_tracks': 27, 'type': 'album', 'uri': 'spotify:album:6BzxX6zkDsYKFJ04ziU5xQ'}, 'artists': [{'external_urls': {'spotify': 'https://open.spotify.com/artist/6vWDO969PvNqNYHIOW5v0m'}, 'href': 'https://api.spotify.com/v1/artists/6vWDO969PvNqNYHIOW5v0m', 'id': '6vWDO969PvNqNYHIOW5v0m', 'name': 'Beyoncé', 'type': 'artist', 'uri': 'spotify:artist:6vWDO969PvNqNYHIOW5v0m'}], 'available_markets': ['AR', 'AU', 'AT', 'BE', 'BO', 'BR', 'BG', 'CA', 'CL', 'CO', 'CR', 'CY', 'CZ', 'DK', 'DO', 'DE', 'EC', 'EE', 'SV', 'FI', 'FR', 'GR', 'GT', 'HN', 'HK', 'HU', 'IS', 'IE', 'IT', 'LV', 'LT', 'LU', 'MY', 'MT', 'MX', 'NL', 'NZ', 'NI', 'NO', 'PA', 'PY', 'PE', 'PH', 'PL', 'PT', 'SG', 'SK', 'ES', 'SE', 'CH', 'TW', 'TR', 'UY', 'US', 'GB', 'AD', 'LI', 'MC', 'ID', 'JP', 'TH', 'VN', 'RO', 'IL', 'ZA', 'SA', 'AE', 'BH', 'QA', 'OM', 'KW', 'EG', 'MA', 'DZ', 'TN', 'LB', 'JO', 'PS', 'IN', 'BY', 'KZ', 'MD', 'UA', 'AL', 'BA', 'HR', 'ME', 'MK', 'RS', 'SI', 'KR', 'BD', 'PK', 'LK', 'GH', 'KE', 'NG', 'TZ', 'UG', 'AG', 'AM', 'BS', 'BB', 'BZ', 'BT', 'BW', 'BF', 'CV', 'CW', 'DM', 'FJ', 'GM', 'GE', 'GD', 'GW', 'GY', 'HT', 'JM', 'KI', 'LS', 'LR', 'MW', 'MV', 'ML', 'MH', 'FM', 'NA', 'NR', 'NE', 'PW', 'PG', 'PR', 'WS', 'SM', 'ST', 'SN', 'SC', 'SL', 'SB', 'KN', 'LC', 'VC', 'SR', 'TL', 'TO', 'TT', 'TV', 'VU', 'AZ', 'BN', 'BI', 'KH', 'CM', 'TD', 'KM', 'GQ', 'SZ', 'GA', 'GN', 'KG', 'LA', 'MO', 'MR', 'MN', 'NP', 'RW', 'TG', 'UZ', 'ZW', 'BJ', 'MG', 'MU', 'MZ', 'AO', 'CI', 'DJ', 'ZM', 'CD', 'CG', 'IQ', 'LY', 'TJ', 'VE', 'ET', 'XK'], 'disc_number': 1, 'duration_ms': 189638, 'explicit': False, 'external_ids': {'isrc': 'USSM12402711'}, 'external_urls': {'spotify': 'https://open.spotify.com/track/2PmMh2t7jAtN6cqFooA0Xy'}, 'href': 'https://api.spotify.com/v1/tracks/2PmMh2t7jAtN6cqFooA0Xy', 'id': '2PmMh2t7jAtN6cqFooA0Xy', 'is_local': False, 'is_playable': True, 'name': 'JOLENE', 'popularity': 63, 'preview_url': None, 'track_number': 10, 'type': 'track', 'uri': 'spotify:track:2PmMh2t7jAtN6cqFooA0Xy'}]}}\n{'external_urls': {'spotify': 'https://open.spotify.com/artist/6vWDO969PvNqNYHIOW5v0m'}, 'followers': {'href': None, 'total': 40922681}, 'genres': [], 'href': 'https://api.spotify.com/v1/artists/6vWDO969PvNqNYHIOW5v0m', 'id': '6vWDO969PvNqNYHIOW5v0m', 'images': [{'url': 'https://i.scdn.co/image/ab6761610000e5eb7eaa373538359164b843f7c0', 'height': 640, 'width': 640}, {'url': 'https://i.scdn.co/image/ab676161000051747eaa373538359164b843f7c0', 'height': 320, 'width': 320}, {'url': 'https://i.scdn.co/image/ab6761610000f1787eaa373538359164b843f7c0', 'height': 160, 'width': 160}], 'name': 'Beyoncé', 'popularity': 88, 'type': 'artist', 'uri': 'spotify:artist:6vWDO969PvNqNYHIOW5v0m'}\nSong: JOLENE\nArtist genres: []\n\n\n\n\nCode\n#after testing, it seems spotify's API is unable to reliably find the genres of songs..."
  }
]